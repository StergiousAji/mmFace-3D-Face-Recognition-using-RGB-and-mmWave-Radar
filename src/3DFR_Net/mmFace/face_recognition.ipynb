{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CUDAExecutionProvider': {'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'device_id': '0', 'gpu_external_alloc': '0', 'enable_cuda_graph': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'cudnn_conv_use_max_workspace': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'tunable_op_enable': '0', 'tunable_op_tuning_enable': '0', 'enable_skip_layer_norm_strict_mode': '0'}, 'CPUExecutionProvider': {}}\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CUDAExecutionProvider': {'do_copy_in_default_stream': '1', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'device_id': '0', 'gpu_external_alloc': '0', 'enable_cuda_graph': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'cudnn_conv_use_max_workspace': '1', 'cudnn_conv1d_pad_to_nc1d': '0', 'tunable_op_enable': '0', 'tunable_op_tuning_enable': '0', 'enable_skip_layer_norm_strict_mode': '0'}, 'CPUExecutionProvider': {}}\n"
     ]
    }
   ],
   "source": [
    "import insightface\n",
    "from insightface.app.common import Face\n",
    "from insightface.model_zoo import model_zoo\n",
    "from neural_nets import MMFaceFE, MMFaceClassifier, insightface_model, InsightFaceClassifier\n",
    "import numpy as np\n",
    "# REQUIRED FOR CUDA TO BE USED\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_subjects = 21\n",
    "\n",
    "det_model = model_zoo.get_model(\"../models/buffalo_l/det_10g.onnx\")\n",
    "det_model.prepare(ctx_id=0, input_size=(480, 640), det_thres=0.5)\n",
    "rec_model = model_zoo.get_model(\"../models/buffalo_l/w600k_r50.onnx\")\n",
    "\n",
    "# TODO: LOAD THESE MODELS WITH TRAINED WEIGHTS\n",
    "mmface_model = MMFaceFE().to(device)\n",
    "mmface_model.eval()\n",
    "\n",
    "insightface_classifier = InsightFaceClassifier(num_subjects).to(device)\n",
    "insightface_classifier.eval()\n",
    "\n",
    "mmface_classifier = MMFaceClassifier().to(device)\n",
    "mmface_classifier.eval()\n",
    "\n",
    "def recognise(rgb_input, radar_input):\n",
    "    with torch.no_grad():\n",
    "        rgb_emb = insightface_model(rgb_input[..., ::-1], det_model, Face, rec_model)\n",
    "        radar_emb = mmface_model(radar_input)\n",
    "    \n",
    "    return rgb_emb, radar_emb\n",
    "\n",
    "\n",
    "def classify(rgb_emb, radar_emb):\n",
    "    with torch.no_grad():\n",
    "        subject = insightface_classifier(rgb_emb)\n",
    "        liveness = mmface_classifier(radar_emb)\n",
    "    \n",
    "    return subject, liveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = \"../../Soli/soli_realsense/data\"\n",
    "\n",
    "def load_rgb(subject, experiment):\n",
    "    return np.load(f\"{DATA_PATH}/{subject}/{subject}-{experiment}_colour.npy\").astype(np.float32)\n",
    "\n",
    "def recognise_rgb(rgb_input):\n",
    "    return insightface_model(rgb_input[..., ::-1], det_model, Face, rec_model)\n",
    "\n",
    "fake_0 = recognise_rgb(load_rgb(90, 0)[0])\n",
    "fake_0_1 = recognise_rgb(load_rgb(90, 5)[0])\n",
    "real_0 = recognise_rgb(load_rgb(0, 0)[0])\n",
    "real_0_1 = recognise_rgb(load_rgb(0, 1)[0])\n",
    "real_1 = recognise_rgb(load_rgb(1, 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3785049319267273 0.0\n"
     ]
    }
   ],
   "source": [
    "score_good = np.clip(np.dot(fake_0_1, fake_0.T), 0., 1.)\n",
    "score_bad = np.clip(np.dot(fake_0, real_1.T), 0., 1.)\n",
    "\n",
    "print(score_good, score_bad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dfr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
