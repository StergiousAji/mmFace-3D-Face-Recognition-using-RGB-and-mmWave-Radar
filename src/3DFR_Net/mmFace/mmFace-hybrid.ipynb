{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mmFace + InsightFace2D Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from utils import by_experiment, get_crd_data\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Extract 10 ARD frames and match with rgb_emb from Experiments 0, 5 and 10 \n",
    "#   - Or duplicate RGB embedding for multiple ARD frames so can extract 75\n",
    "\n",
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, radar_data, rgb_embs, subject_labels, liveness_labels, split):\n",
    "        self.radar_data = radar_data\n",
    "        self.rgb_embs = rgb_embs\n",
    "        self.subject_labels = subject_labels\n",
    "        self.liveness_labels = liveness_labels\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.radar_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.radar_data[idx], self.rgb_embs[idx], self.subject_labels[idx], self.liveness_labels[idx]\n",
    "\n",
    "\n",
    "def get_ard(path, subject, experiment, num_frames):\n",
    "        with open(f\"{path}/{subject}-{experiment}_radar.json\", 'r') as f:\n",
    "            ard = np.abs(get_crd_data(json.load(f), num_chirps_per_burst=16))[:num_frames].astype(np.float32)\n",
    "            return ard[np.random.permutation(len(ard))]            \n",
    "\n",
    "def load_dataset(raw_path, num_subjects, num_experiments=15, train_split=0.8, test_split=0.1, device=\"cuda\", seed=24):\n",
    "    np.random.seed(seed)\n",
    "    subjects = list(range(num_subjects)) + [int(f\"9{s}\") for s in range(num_subjects)]\n",
    "    experiments = range(num_experiments)\n",
    "    val_split_end = train_split + test_split\n",
    "\n",
    "    data = {\"radar\": [[], [], []], \"rgb_embs\": [[], [], []]}\n",
    "    labels = {\"subject\": [[], [], []], \"liveness\": [[], [], []]}\n",
    "\n",
    "    # Subjects x Experiments x Frames x Embedding_Size *(21 x 15 x 10 x 512)\n",
    "    real_rgb = np.load(\"data/InsightFace_embs/real_insightface_embs.npy\")\n",
    "    # BE CAREFUL THIS IS MOSTLY ZEROS\n",
    "    fake_rgb = np.load(\"data/InsightFace_embs/fake_insightface_embs.npy\")\n",
    "\n",
    "    # TODO: FILTER UNDETECTED EMBEDDINGS\n",
    "    for subject in subjects:\n",
    "        num_frames = 250 if subject < 90 else 74\n",
    "        live = 0 if subject < 90 else 1\n",
    "        experiments = range(num_experiments) if subject < 90 else [0, 5, 10]\n",
    "        for experiment in experiments:\n",
    "            exp_ard = get_ard(raw_path, subject, experiment, num_frames)\n",
    "            n = 15 if subject < 90 else len(exp_ard)\n",
    "\n",
    "            exp_train = exp_ard[:int(n*train_split)]\n",
    "            exp_val = exp_ard[int(n*train_split):int(n*val_split_end)]\n",
    "            exp_test = exp_ard[int(n*val_split_end):n]\n",
    "\n",
    "            data[\"radar\"][0].append(exp_train)\n",
    "            data[\"radar\"][1].append(exp_val)\n",
    "            data[\"radar\"][2].append(exp_test)\n",
    "\n",
    "            data[\"rgb_embs\"][0].append(rgb_train)\n",
    "            data[\"rgb_embs\"][1].append(rgb_val)\n",
    "            data[\"rgb_embs\"][2].append(rgb_test)\n",
    "\n",
    "            labels[\"subject\"][0].append([subject]*len(exp_train))\n",
    "            labels[\"subject\"][1].append([subject]*len(exp_val))\n",
    "            labels[\"subject\"][2].append([subject]*len(exp_test))\n",
    "\n",
    "            labels[\"liveness\"][0].append([live]*len(exp_train))\n",
    "            labels[\"liveness\"][1].append([live]*len(exp_val))\n",
    "            labels[\"liveness\"][2].append([live]*len(exp_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_subjects = 21\n",
    "\n",
    "train, validation, test = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets import MMFaceHybrid\n",
    "from utils import load_model, load_history\n",
    "from torch import nn\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "lambda1 = 1\n",
    "lambda2 = 1\n",
    "\n",
    "model = MMFaceHybrid(num_subjects).to(device)\n",
    "\n",
    "# Loss + Optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=0.001, momentum=0.9)\n",
    "\n",
    "model_name = f\"mmFace-hybrid.pt\"\n",
    "cur_epoch, loss_history, train_acc, val_acc = load_model(model_name, model, optimiser)\n",
    "\n",
    "if len(loss_history) > 0:\n",
    "    print(f\"{model_name}\\n\\tEpoch: {cur_epoch}\\n\\tLoss: {loss_history[-1]:.4f}\\n\\tTrain Accuracy: {train_acc[-1]:.4f}\\n\\tValidation Accuracy: {val_acc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(cur_epoch, num_epochs):\n",
    "    print(f\"\\nEpoch [{epoch}/{num_epochs-1}]:\")\n",
    "    if os.path.exists(f\"models/{model_name}\"):\n",
    "        loss_history, train_acc, val_acc = load_history(f\"models/{model_name}\")\n",
    "\n",
    "    model.train()\n",
    "    # Running Loss and Accuracy\n",
    "    running_loss, running_acc_s, running_acc_l, total_s, total_l = 0., 0., 0., 0., 0.\n",
    "\n",
    "    for radar, rgb_emb, labels_s, labels_l in tqdm(train):\n",
    "        # Forward Pass\n",
    "        out1, out2 = model(radar, rgb_emb)\n",
    "        _, preds_s = torch.max(out1.data, 1)\n",
    "        _, preds_l = torch.max(out2.data, 1)\n",
    "        loss1 = criterion(out1, labels_s)\n",
    "        loss2 = criterion(out2, labels_l)\n",
    "        loss = lambda1*loss1 + lambda2*loss2\n",
    "\n",
    "        # Backward Pass and Optimise\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_s += labels_s.size(0)\n",
    "        total_l += labels_l.size(0)\n",
    "        running_acc_s += (preds_s == labels_s).sum().item()\n",
    "        running_acc_l += (preds_l == labels_l).sum().item()\n",
    "\n",
    "        del radar, rgb_emb, labels_s, labels_l, out1, out2\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    avg_train_loss = running_loss/len(train)\n",
    "    avg_train_acc_s = 100*running_acc_s/total\n",
    "    avg_train_acc_l = 100*running_acc_l/total\n",
    "    print(f\"\\tAverage Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"\\tTrain Accuracy (Subjects): {avg_train_acc_s:.4f}%\")\n",
    "    print(f\"\\tTrain Accuracy (Liveness): {avg_train_acc_l:.4f}%\")\n",
    "\n",
    "    torch.save({\"epoch\": epoch+1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimiser_state_dict\": optimiser.state_dict(),\n",
    "                \"loss_history\": loss_history + [avg_train_loss],\n",
    "                \"train_acc\": train_acc + [(avg_train_acc_s, avg_train_acc_l)],\n",
    "                \"val_acc\": val_acc},\n",
    "                f\"models/{model_name}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_s, correct_l, total_s, total_l = 0., 0., 0., 0.\n",
    "        for radar, rgb_emb, labels_s, labels_l in validation:\n",
    "            out1, out2 = model(radar, rgb_emb)\n",
    "            _, preds_s = torch.max(out1.data, 1)\n",
    "            _, preds_l = torch.max(out2.data, 1)\n",
    "            total_s += labels_s.size(0)\n",
    "            total_l += labels_l.size(0)\n",
    "            correct_s += (preds_s == labels_s).sum().item()\n",
    "            correct_l += (preds_l == labels_l).sum().item()\n",
    "            del radar, rgb_emb, labels_s, labels_l, out1, out2\n",
    "        \n",
    "        avg_val_acc_s = 100*correct_s/total_s\n",
    "        avg_val_acc_l = 100*correct_l/total_l\n",
    "        print(f\"\\tValidation Accuracy (Subject): {avg_val_acc_s:.4f}%\")\n",
    "        print(f\"\\tValidation Accuracy (Liveness): {avg_val_acc_l:.4f}%\")\n",
    "\n",
    "    model_checkpoint = torch.load(f\"models/{model_name}\")\n",
    "    model_checkpoint[\"val_acc\"].append((avg_val_acc_s, avg_val_acc_l))\n",
    "    torch.save(model_checkpoint, f\"models/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_subject, preds_liveness = [], []\n",
    "true_subject, true_liveness = [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct_s, correct_l, total_s, total_l = 0., 0., 0., 0.\n",
    "    for radar, rgb_emb, labels_s, labels_l in validation:\n",
    "        out1, out2 = model(radar, rgb_emb)\n",
    "        _, preds_s = torch.max(out1.data, 1)\n",
    "        _, preds_l = torch.max(out2.data, 1)\n",
    "        total_s += labels_s.size(0)\n",
    "        total_l += labels_l.size(0)\n",
    "        correct_s += (preds_s == labels_s).sum().item()\n",
    "        correct_l += (preds_l == labels_l).sum().item()\n",
    "        del radar, rgb_emb, labels_s, labels_l, out1, out2\n",
    "\n",
    "        preds_subject.extend(preds_s.cpu().numpy())\n",
    "        preds_liveness.extend(preds_l.cpu().numpy())\n",
    "        true_subject.extend(labels_s.data.cpu().numpy())\n",
    "        true_liveness.extend(labels_l.data.cpu().numpy())\n",
    "        del radar, rgb_emb, labels_s, labels_l, out1, out2\n",
    "    \n",
    "    print(f\"Test Accuracy (Subject): {100*correct_s/total_s:.4f}%\")\n",
    "    print(f\"Test Accuracy (Liveness): {100*correct_l/total_l:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "losses, train_acc, val_acc = load_history(f\"models/{model_name}\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3), dpi=120)\n",
    "\n",
    "axs[0].plot(range(len(train_acc)), train_acc)\n",
    "axs[0].plot(range(len(val_acc)), val_acc)\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "axs[1].plot(range(len(losses)), losses)\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "axs[0].set_title(model_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(preds_subject, true_subject)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index=subject_names, columns=subject_names)\n",
    "plt.figure(figsize = (14, 5))\n",
    "heatmap = sn.heatmap(df_cm, annot=True)\n",
    "heatmap.set(xlabel =\"Predictions\", ylabel = \"Actual\", title ='Precision Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liveness Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(preds_liveness, true_liveness)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index=[\"Real\", \"Fake\"], columns=[\"Real\", \"Fake\"])\n",
    "plt.figure(figsize = (14, 5))\n",
    "heatmap = sn.heatmap(df_cm, annot=True)\n",
    "heatmap.set(xlabel =\"Predictions\", ylabel = \"Actual\", title ='Precision Confusion Matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dfr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
